model: transformer
learning_rate: 1e-4
#lr_policy: multistep
#lr_decay_milestones: [50,100]
lr_policy: LinearWarmupCosineAnnealing
dins: "69,85"
douts: "69"
input_modalities: "expmap_scaled_20,audio_feats_scaled_20"
output_modalities: "expmap_scaled_20"
input_lengths: "120,160"
output_lengths: "3"
output_time_offsets: "120"
nlayers: 12
optimizer: adam
nhead: 10
dhid: 1200
dropout: 0
#use_pos_emb_output: True
batch_size: 32
gradient_clip_val: 0.5
#precision: 16
#plugins: deepspeed
