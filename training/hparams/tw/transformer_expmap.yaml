model: transformer
learning_rate: 1e-4
lr_policy: multistep
lr_decay_milestones: [50,100]
dins: 256,125,8
douts: 8
input_modalities: "annotation,npz.obs_scaled,npz.acts_scaled"
output_modalities: npz.acts_scaled
input_lengths: 11,60,60
output_lengths: 1
output_time_offsets: 60
input_proc_types: single,none,none
input_types: d,c,c
output_proc_types: none
#output_types: c
input_num_tokens: 67,0,0
nlayers: 12
optimizer: adam
nhead: 10
dhid: 800
dropout: 0
use_rel_pos_emb_inputs: True
#use_pos_emb_output: True
batch_size: 128
gradient_clip_val: 10
#precision: 16
#plugins: deepspeed
